{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "eltazr3-syn"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/001_create_schemas')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = 'stg') EXEC('CREATE SCHEMA stg');\nIF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = 'core') EXEC('CREATE SCHEMA core');\nIF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = 'mdl') EXEC('CREATE SCHEMA mdl');\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "eltazr3_sqlpool",
						"poolName": "eltazr3_sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/010_create_db_roles')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT 1 FROM sys.database_principals WHERE name='role_ingest_loader')\n    CREATE ROLE role_ingest_loader;\n\n-- Allow loaders to write/read staging\nGRANT SELECT, INSERT, UPDATE, DELETE ON SCHEMA::stg TO role_ingest_loader;\n-- If you’ll let ADF auto-create staging tables at first:\nGRANT ALTER, CONTROL ON SCHEMA::stg TO role_ingest_loader;  -- or use db_ddladmin temporarily\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "eltazr3_sqlpool",
						"poolName": "eltazr3_sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/030_bind_loader_principal')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Variables passed at deploy time:\n--   :setvar ADF_PRINCIPAL \"eltazr3-adf\"   -- or an Entra group like 'dp-adf-loaders-dev'\n-- If you prefer objectId: use the GUID as the username string.\n\nIF NOT EXISTS (SELECT 1 FROM sys.database_principals WHERE name = '$(ADF_PRINCIPAL)')\n    CREATE USER [$(ADF_PRINCIPAL)] FROM EXTERNAL PROVIDER;\n\n-- Either attach to custom role...\nEXEC sp_addrolemember N'role_ingest_loader', N'$(ADF_PRINCIPAL)';\n\n-- ...or use built-in roles if you prefer (comment one style out):\n-- ALTER ROLE db_datareader ADD MEMBER [$(ADF_PRINCIPAL)];\n-- ALTER ROLE db_datawriter ADD MEMBER [$(ADF_PRINCIPAL)];\n-- -- Temporary while bootstrapping table creation:\n-- ALTER ROLE db_ddladmin  ADD MEMBER [$(ADF_PRINCIPAL)];\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "eltazr3_sqlpool",
						"poolName": "eltazr3_sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/100_stg_trip')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF OBJECT_ID('stg.trip') IS NULL\nBEGIN\n    CREATE TABLE stg.trip\n    (\n        -- TODO: add source columns 1:1...\n        -- vendor_id       VARCHAR(10) NULL,\n        -- tpep_pickup_datetime  DATETIME2 NULL,\n        -- ...etc...\n\n        ingest_date        DATE        NOT NULL,\n        source_file_name   NVARCHAR(512) NOT NULL,\n        loaded_at          DATETIME2   NOT NULL\n    )\n    WITH (DISTRIBUTION = ROUND_ROBIN, HEAP);\nEND;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "eltazr3_sqlpool",
						"poolName": "eltazr3_sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_convert_parquet_to_snappy')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spsmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "35803d95-e9f8-4a80-bdec-8405197d5668"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/6c6119b6-a5fd-406b-aa20-45a34495a8d7/resourceGroups/eltazr3-rg/providers/Microsoft.Synapse/workspaces/eltazr3-syn/bigDataPools/spsmall",
						"name": "spsmall",
						"type": "Spark",
						"endpoint": "https://eltazr3-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spsmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# === EDIT THESE FOR YOUR RUN ===\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"account       = \"eltazr3adls\"                 # ADLS account\n",
							"file_system   = \"raw\"                         # container\n",
							"dataset       = \"nyctaxi_yellow\"\n",
							"ingest_date   = \"2025-07-31\"\n",
							"src_file_name = \"yellow_tripdata_2025-07.parquet\"          # ZSTD input\n",
							"tgt_file_name = \"yellow_tripdata_2025-07.snappy.millis.parquet\"   # Snappy output\n",
							"\n",
							"# ---- derived paths (don’t edit) ----\n",
							"base_dir = f\"abfss://{file_system}@{account}.dfs.core.windows.net/{dataset}/ingest_date={ingest_date}\"\n",
							"src_path = f\"{base_dir}/{src_file_name}\"\n",
							"tgt_path = f\"{base_dir}/{tgt_file_name}\"\n",
							"\n",
							"print(\"Source:\", src_path)\n",
							"print(\"Target:\", tgt_path)\n",
							"\n",
							"# Guardrails\n",
							"if src_file_name == tgt_file_name:\n",
							"    raise ValueError(\"src_file_name and tgt_file_name are identical; choose a different target name\")\n",
							"if not tgt_file_name.endswith(\".parquet\"):\n",
							"    raise ValueError(\"tgt_file_name must end with .parquet\")\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"skip = False\n",
							"\n",
							"# If target already exists, we skip conversion but can still (later) write a manifest if missing.\n",
							"if mssparkutils.fs.exists(tgt_path):\n",
							"    print(f\"SKIP: target already exists → {tgt_path}\")\n",
							"    skip = True\n",
							"\n",
							"# Source must exist unless we're skipping\n",
							"if not skip and not mssparkutils.fs.exists(src_path):\n",
							"    raise FileNotFoundError(f\"Source file not found: {src_path}\")\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"if skip:\n",
							"    print(\"SKIP: read step (target exists).\")\n",
							"else:\n",
							"    df = spark.read.parquet(src_path)\n",
							"    print(\"Rows:\", df.count())\n",
							"    df.printSchema()\n",
							"\n",
							"    from pyspark.sql.functions import date_format, col\n",
							"\n",
							"    # keep UTC semantics to avoid time shifts\n",
							"    spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
							"\n",
							"    # convert problem columns to ISO-like strings ADF can cast to datetime2\n",
							"    TS_FMT = \"yyyy-MM-dd HH:mm:ss\"   # safe for Synapse datetime2\n",
							"    cols_to_string = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
							"\n",
							"    for c in cols_to_string:\n",
							"        if c in df.columns:\n",
							"            df = df.withColumn(c, date_format(col(c), TS_FMT))\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"if skip:\n",
							"    print(\"SKIP: write step (target exists).\")\n",
							"else:\n",
							"    from uuid import uuid4\n",
							"    tmp_dir = f\"{base_dir}/_tmp_snappy_{uuid4().hex}\"\n",
							"\n",
							"    # Clean stale tmp dir if present (rare)\n",
							"    if mssparkutils.fs.exists(tmp_dir):\n",
							"        mssparkutils.fs.rm(tmp_dir, recurse=True)\n",
							"\n",
							"    spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
							"\n",
							"    # ensure UTC semantics and Parquet physical type = TIMESTAMP_MILLIS\n",
							"    spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
							"    spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MILLIS\")\n",
							"\n",
							"    (df.coalesce(1)\n",
							"       .write\n",
							"       .mode(\"error\")    # error if tmp exists\n",
							"       .parquet(tmp_dir))\n",
							"\n",
							"    print(\"Temp dir:\", tmp_dir)\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"if skip:\n",
							"    print(\"SKIP: rename step (target exists).\")\n",
							"else:\n",
							"    parts = [f.path for f in mssparkutils.fs.ls(tmp_dir) if f.path.lower().endswith(\".parquet\")]\n",
							"    if len(parts) != 1:\n",
							"        # Clean up then error to avoid leaks\n",
							"        for f in mssparkutils.fs.ls(tmp_dir):\n",
							"            mssparkutils.fs.rm(f.path, recurse=True)\n",
							"        mssparkutils.fs.rm(tmp_dir, recurse=True)\n",
							"        raise RuntimeError(f\"Expected 1 part file, found {len(parts)}: {parts}\")\n",
							"\n",
							"    part_path = parts[0]\n",
							"    print(\"Part file:\", part_path)\n",
							"    print(\"Target   :\", tgt_path)\n",
							"\n",
							"    # Final guard: if target suddenly exists, skip and clean temp\n",
							"    if mssparkutils.fs.exists(tgt_path):\n",
							"        for f in mssparkutils.fs.ls(tmp_dir):\n",
							"            mssparkutils.fs.rm(f.path, recurse=True)\n",
							"        mssparkutils.fs.rm(tmp_dir, recurse=True)\n",
							"        print(f\"SKIP: target appeared during run → {tgt_path}\")\n",
							"    else:\n",
							"        mssparkutils.fs.mv(part_path, tgt_path)\n",
							"        # Remove remaining tmp artifacts (_SUCCESS, etc.)\n",
							"        for f in mssparkutils.fs.ls(tmp_dir):\n",
							"            mssparkutils.fs.rm(f.path, recurse=True)\n",
							"        mssparkutils.fs.rm(tmp_dir, recurse=True)\n",
							"        print(\"Wrote:\", tgt_path)\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# === Cell 6 (revised): Write manifest for the Snappy file (no overwrite) ===\n",
							"import json, hashlib\n",
							"from uuid import uuid4\n",
							"from datetime import datetime, timezone\n",
							"from pyspark.sql import functions as F\n",
							"\n",
							"# Ensure target exists (either from skip or from this run)\n",
							"if not mssparkutils.fs.exists(tgt_path):\n",
							"    raise FileNotFoundError(f\"Expected target not found: {tgt_path}\")\n",
							"\n",
							"# Size from listing\n",
							"info = mssparkutils.fs.ls(tgt_path)[0]\n",
							"size_bytes = info.size\n",
							"\n",
							"# Try to compute MD5 using Spark's binaryFile reader\n",
							"md5_hex = \"UNAVAILABLE\"\n",
							"try:\n",
							"    # Some runtimes accept a single-file path directly; if not, fall back to loading the folder and filtering\n",
							"    try:\n",
							"        bf = spark.read.format(\"binaryFile\").load(tgt_path)\n",
							"    except Exception:\n",
							"        bf = (spark.read.format(\"binaryFile\")\n",
							"              .load(base_dir)\n",
							"              .where(F.col(\"path\") == tgt_path))\n",
							"    row = bf.select(\"content\").head()\n",
							"    if row is not None and row.content is not None:\n",
							"        md5_hex = hashlib.md5(row.content).hexdigest()\n",
							"        print(\"MD5 computed via binaryFile.\")\n",
							"    else:\n",
							"        print(\"binaryFile returned no content; MD5 set to UNAVAILABLE.\")\n",
							"except Exception as e:\n",
							"    print(\"MD5 computation skipped:\", e)\n",
							"\n",
							"ts_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
							"manifest_path = f\"{base_dir}/_INGESTION_{tgt_file_name}.json\"\n",
							"\n",
							"manifest = {\n",
							"  \"dataset\": dataset,\n",
							"  \"ingest_date\": ingest_date,\n",
							"  \"file\": tgt_file_name,\n",
							"  \"bytes\": size_bytes,\n",
							"  \"md5\": md5_hex,\n",
							"  \"source\": \"synapse-spark zstd→snappy\",\n",
							"  \"run_id\": str(uuid4()),\n",
							"  \"run_by\": \"synapse-mi\",\n",
							"  \"status\": \"SUCCEEDED\",\n",
							"  \"ts_utc\": ts_utc\n",
							"}\n",
							"\n",
							"if mssparkutils.fs.exists(manifest_path):\n",
							"    print(\"Manifest already exists:\", manifest_path)\n",
							"else:\n",
							"    mssparkutils.fs.put(manifest_path, json.dumps(manifest, indent=2), overwrite=False)\n",
							"    print(\"Manifest written:\", manifest_path)\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spsmall')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eltazr3_sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}