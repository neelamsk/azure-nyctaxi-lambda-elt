{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "eltazr1-syn"
		},
		"eltazr1-syn-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'eltazr1-syn-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:eltazr1-syn.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"eltazr1-syn-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://eltazr1adls.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/eltazr1-syn-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('eltazr1-syn-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eltazr1-syn-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('eltazr1-syn-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spsmall01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5641f067-032a-4cef-8de7-8f78754d00dc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/6a3f84f3-29c7-4c8e-b826-7998c044dc9d/resourceGroups/eltazr1-rg/providers/Microsoft.Synapse/workspaces/eltazr1-syn/bigDataPools/spsmall01",
						"name": "spsmall01",
						"type": "Spark",
						"endpoint": "https://eltazr1-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spsmall01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Synapse: read ADF Notebook activity parameters via job tags.\n",
							"# When you run manually in Studio, it falls back to the defaults below.\n",
							"\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"def get_param(name: str, default: str):\n",
							"    try:\n",
							"        v = mssparkutils.env.getJobTag(name)  # set by ADF baseParameters\n",
							"        if v and v.lower() != \"null\":\n",
							"            return v\n",
							"    except Exception:\n",
							"        pass\n",
							"    return default\n",
							"\n",
							"# ---- set defaults used only when running interactively in Studio ----\n",
							"ingest_date = get_param(\"ingest_date\", \"2025-07-01\")\n",
							"file_name   = get_param(\"file_name\",   \"yellow_tripdata_2025-07.parquet\")\n",
							"\n",
							"print(\"ingest_date:\", ingest_date)\n",
							"print(\"file_name  :\", file_name)\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"acct = \"eltazr1adls\"  # your default workspace storage account\n",
							"test_dir = f\"abfss://raw@{acct}.dfs.core.windows.net/_syn_auth_test/\"\n",
							"\n",
							"# Create/list a tiny file. If this fails with auth, fix RBAC on the storage account:\n",
							"# grant Storage Blob Data Contributor to the Synapse workspace's managed identity.\n",
							"mssparkutils.fs.mkdirs(test_dir)\n",
							"mssparkutils.fs.put(test_dir + \"ok.txt\", \"hello\", True)\n",
							"display(mssparkutils.fs.ls(test_dir))\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# === DROP-IN: Transcode to Snappy (with decimal widening + cleanup) ===\n",
							"from notebookutils import mssparkutils\n",
							"from pyspark.sql.types import DecimalType\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"# Get parameters from ADF (falls back to defaults when run manually)\n",
							"def get_param(name, default):\n",
							"    try:\n",
							"        v = mssparkutils.env.getJobTag(name)\n",
							"        if v and v.lower() != \"null\":\n",
							"            return v\n",
							"    except Exception:\n",
							"        pass\n",
							"    return default\n",
							"\n",
							"acct = \"eltazr1adls\"\n",
							"ingest_date = get_param(\"ingest_date\", \"2025-07-01\")\n",
							"file_name   = get_param(\"file_name\",   \"yellow_tripdata_2025-07.parquet\")\n",
							"\n",
							"src = f\"abfss://raw@{acct}.dfs.core.windows.net/nyc_taxi/ingest_date={ingest_date}/{file_name}\"\n",
							"dst = f\"abfss://raw@{acct}.dfs.core.windows.net/nyc_taxi_snappy/ingest_date={ingest_date}/file_name={file_name}/\"\n",
							"\n",
							"# Optional self-skip if output already exists\n",
							"if mssparkutils.fs.exists(dst):\n",
							"    try:\n",
							"        if len(mssparkutils.fs.ls(dst)) > 0:\n",
							"            print(f\"[SKIP] Snappy already present for {file_name} on {ingest_date}: {dst}\")\n",
							"            mssparkutils.notebook.exit(\"SKIP\")\n",
							"    except Exception:\n",
							"        pass\n",
							"\n",
							"# Read original parquet\n",
							"df = spark.read.parquet(src)\n",
							"\n",
							"# ---- widen DECIMALs so Synapse COPY doesn't choke (INT64â†’Binary) ----\n",
							"def widen_decimals_for_synapse(df):\n",
							"    cols = []\n",
							"    for f in df.schema.fields:\n",
							"        c = col(f.name)\n",
							"        if isinstance(f.dataType, DecimalType) and f.dataType.precision <= 18:\n",
							"            c = c.cast(\"decimal(38,10)\")\n",
							"        cols.append(c.alias(f.name))\n",
							"    return df.select(*cols)\n",
							"\n",
							"df = widen_decimals_for_synapse(df)\n",
							"\n",
							"# Write as Snappy (idempotent)\n",
							"(df.coalesce(1)\n",
							"   .write\n",
							"   .mode(\"overwrite\")\n",
							"   .option(\"compression\", \"snappy\")\n",
							"   .parquet(dst))\n",
							"\n",
							"# Remove non-parquet artifacts (e.g., _SUCCESS)\n",
							"for f in mssparkutils.fs.ls(dst):\n",
							"    name = f.name if hasattr(f, \"name\") else f.path.rsplit(\"/\", 1)[-1]\n",
							"    if not name.lower().endswith(\".parquet\"):\n",
							"        try:\n",
							"            mssparkutils.fs.rm(f.path, recurse=True)\n",
							"            print(\"Deleted:\", f.path)\n",
							"        except Exception as e:\n",
							"            print(\"Skip delete\", f.path, \"->\", e)\n",
							"\n",
							"print(\"[OK] Wrote:\", dst)\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spsmall01')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eltazr1_sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}